{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Kubernetes and TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We`ll deploy the clothes classification model we trained previously using Kubernetes and TensorFlow Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What we'll cover this week\n",
    "- Two-tier architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kubernetes overview](./images/kubernetes_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The saved_model format\n",
    "- Running TF-Serving locally with Docker\n",
    "- Invoking the model from Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:06:36.359547: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-09-19 16:06:36.359598: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-09-19 16:06:36.359607: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-09-19 16:06:36.359659: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-19 16:06:36.359701: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = keras.models.load_model(\"./models/xception_v4_08_0.868.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/clothing-model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/clothing-model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, \"./models/clothing-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m\u001b[39m\u001b[38;5;6m\u001b[1m clothing-model\u001b[0m\n",
      "\u001b[38;5;245m├── \u001b[39m\u001b[38;5;6m\u001b[1m assets\u001b[0m\n",
      "\u001b[38;5;245m├── \u001b[39m fingerprint.pb\n",
      "\u001b[38;5;245m├── \u001b[39m saved_model.pb\n",
      "\u001b[38;5;245m└── \u001b[39m\u001b[38;5;6m\u001b[1m variables\u001b[0m\n",
      "\u001b[38;5;245m    ├── \u001b[39m variables.data-00000-of-00001\n",
      "\u001b[38;5;245m    └── \u001b[39m variables.index\n"
     ]
    }
   ],
   "source": [
    "!lsd --tree models/clothing-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_17'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 299, 299, 3)\n",
      "        name: serving_default_input_17:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_14'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "The MetaGraph with tag set ['serve'] contains the following ops: {'Const', 'Mean', 'StatefulPartitionedCall', 'Placeholder', 'SaveV2', 'Identity', 'DisableCopyOnRead', 'ShardedFilename', 'Pack', 'AssignVariableOp', 'StringJoin', 'RestoreV2', 'StaticRegexFullMatch', 'MaxPool', 'NoOp', 'BiasAdd', 'Conv2D', 'Select', 'MatMul', 'VarHandleOp', 'MergeV2Checkpoints', 'AddV2', 'ReadVariableOp', 'FusedBatchNormV3', 'Relu', 'DepthwiseConv2dNative'}\n",
      "2024-09-19 16:13:37.144308: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-09-19 16:13:37.144334: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-09-19 16:13:37.144339: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-09-19 16:13:37.144365: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-19 16:13:37.144380: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_17: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_17')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_17: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_17')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_17: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_17')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_17: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_17')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_17: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_17')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir models/clothing-model --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the second signature defined in the model, which is the one that takes an image as input and returns the class probabilities. see [Model description](./model-description.txt)\n",
    "\n",
    "Now we have to inject the parameters into a docker container\n",
    "\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "    -p 8500:8500 \\\n",
    "    -v \"$(pwd)/models/clothing-model:/models/clothing-model/1\" \\\n",
    "    -e MODEL_NAME=\"clothing-model\" \\\n",
    "    tensorflow/serving:2.7.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ! HELP: Does not run under Apple Silicon -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-image-helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost:8500'\n",
    "\n",
    "channel = grpc.insecure_channel(host)\n",
    "\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_image_helper import create_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = create_preprocessor('xception', target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://bit.ly/mlbookcamp-pants'\n",
    "X = preprocessor.from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_protobuf(data):\n",
    "    return tf.make_tensor_proto(data, shape=data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_request = predict_pb2.PredictRequest()\n",
    "\n",
    "pb_request.model_spec.name = 'clothing-model'\n",
    "pb_request.model_spec.signature_name = 'serving_default'\n",
    "\n",
    "pb_request.inputs[\"input_17\"].CopyFrom(np_to_protobuf(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_response = stub.Predict(pb_request, timeout=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pb_response.outputs['dense_14'].float_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(classes, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Creating a pre-processing service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting the notebook to a Python script\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to script 10_kubernetes.ipynb\n",
    "```\n",
    "=> saved as [gateway.py](./gateway.py)\n",
    "\n",
    "\n",
    "- Wrappping the script into a Flask app\n",
    "- Testing the service with [`test.py`](./test.py)\n",
    "\n",
    "```bash\n",
    "python test.py\n",
    "```\n",
    "\n",
    "- Putting everything into Pipenv\n",
    "\n",
    "```bash\n",
    "pip install pipenv\n",
    "pipenv install grpcio==1.42.0 flask gunicorn keras-image-helper\n",
    "pipenv install tensorflow-protobuf ==2.7.0\n",
    "pipenv shell\n",
    "python gateway.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not drag 1,2GB TF into docker, we only need `TensorFlow Protobuf`, new file `proto.py`\n",
    "- [https://github.com/alexeygrigorev/tensorflow-protobuf](https://github.com/alexeygrigorev/tensorflow-protobuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Running everything locally with Docker-compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preparing tensorflow-serving image\n",
    "```bash\n",
    "docker build -t zoomcamp-10-model:xception-v4-001 -f image-model.dockerfile .\n",
    "````\n",
    "\n",
    "- Running tensorflow-serving image\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "    -p 8500:8500 \\\n",
    "    zoomcamp-10-model:xception-v4-001\n",
    "```\n",
    "\n",
    "- Running the service, switch __main__ code in `gateway.py`\n",
    "```bash\n",
    "pipenv run python gateway.py\n",
    "```\n",
    "\n",
    "- Building the gateway image\n",
    "```bash\n",
    "docker build -t zoomcamp-10-gateway:001 -f image-gateway.dockerfile .\n",
    "```\n",
    "\n",
    "- Running the gateway\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "    -p 9696:9696 \\\n",
    "    zoomcamp-10-gateway:001\n",
    "```\n",
    "\n",
    "communication between the two containers is done via docker compose -> network bridge\n",
    "\n",
    "- Installing docker-compose on macOS with Homebrew, for other OS see [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)\n",
    "\n",
    "<img src=\"./images/docker_compose.png\" alt=\"docker-compose\" style=\"width:600px;height:auto;\">\n",
    "\n",
    "```bash\n",
    "brew install docker-compose\n",
    "```\n",
    "\n",
    "- Create `/bin`-folder in home directory\n",
    "- move into folder, download docker-compose\n",
    "- make it executable\n",
    "```bash\n",
    "mkdir ~/bin\n",
    "cd ~/bin\n",
    "curl -L <LINK> -o docker-compose\n",
    "chmod +x docker-compose\n",
    "```\n",
    "\n",
    "- Add to PATH\n",
    "```bash\n",
    "echo 'export PATH=\"${HOME}/bin:${PATH}\"' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "- Create `docker-compose.yml`\n",
    "```bash\n",
    "docker build -t zoomcamp-10-gateway:002 -f image-gateway.dockerfile .\n",
    "```\n",
    "\n",
    "- Running the service\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "- Testing the service\n",
    "```bash\n",
    "python test.py\n",
    "```\n",
    "\n",
    "- Ctrl+C to stop the service\n",
    "- detaching the service\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "- Stopping the service\n",
    "```bash\n",
    "docker-compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Introduction to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The anatomy of a Kubernetes cluster\n",
    "\n",
    "<img src=\"./images/intro_kubernetes.png\" alt=\"intro kubernetes\" style=\"width:600px;height:auto;\">\n",
    "\n",
    "<img src=\"./images/glossar.png\" alt=\"glossar\" style=\"width:600px;height:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Deploying a simple service to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create simple ping application in Flask\n",
    "```bash\n",
    "pipenv install flask gunicorn\n",
    "```\n",
    "\n",
    "✘ Locking Failed! Failed to lock Pipfile.lock!\n",
    "on macOS\n",
    "\n",
    "- Installing kubectl - installed with docker/docker-desktop\n",
    "\n",
    "- Setting up a local Kubernetes cluster with Kind\n",
    "```bash\n",
    "brew install kind\n",
    "kind create cluster\n",
    "kubectl cluster-info --context kind-kind\n",
    "kubectl get service\n",
    "kubectl get pods\n",
    "kubectl get deployments\n",
    "```\n",
    "\n",
    "- Creating a deployment\n",
    "    - [deployment.yaml](./deployment.yaml)\n",
    "```bash\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl get deployment\n",
    "kubectl get pod\n",
    "kubectl describe pod <pod-name>\n",
    "kind load docker-image ping:v001\n",
    "kubectl get pod\n",
    "kubectl port-forward <pod-name> 9696:9696\n",
    "curl localhost:9696/ping\n",
    "```\n",
    "\n",
    "- Creating a service\n",
    "  - [service.yaml](./service.yaml)\n",
    "```bash\n",
    "kubectl apply -f service.yaml\n",
    "kubectl get service\n",
    "kubectl get svc\n",
    "```\n",
    "\n",
    "- change service type to `LoadBalancer` in `service.yaml`\n",
    "```bash\n",
    "kubectl apply -f service.yaml\n",
    "kubectl get service\n",
    "kubectl port-forward service/ping 8080:80\n",
    "curl localhost:8080/ping\n",
    "```\n",
    "\n",
    "<img src=\"./images/port_forwarding.png\" alt=\"port forwarding\" style=\"width:600px;height:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Deploying TensorFlow models to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deploying the TF-Serving model\n",
    "\n",
    "```bash\n",
    "kind load docker-image zoomcamp-10-model:xception-v4-001\n",
    "cd kube-config\n",
    "kubectl apply -f model-deployment.yaml\n",
    "kubectl get pod\n",
    "kubectl port-forward <pod-name> 8500:8500\n",
    "kubectl apply -f model-service.yaml\n",
    "kubectl get service\n",
    "kubectl port-forward service/tf-serving-clothing-model 8500:8500\n",
    "```\n",
    "\n",
    "- Deploying the Gateway\n",
    "\n",
    "```bash\n",
    "kind load docker-image zoomcamp-10-gateway:002\n",
    "kubectl apply -f gateway-deployment.yaml\n",
    "```\n",
    "\n",
    "- Testing the service\n",
    "```bash\n",
    "kubectl get pod\n",
    "kubectl exec -it <pod-name> -- /bash\n",
    "```\n",
    "\n",
    "- on pod\n",
    "```bash\n",
    "curl localhost:9696/ping\n",
    "```\n",
    "\n",
    "- Exposing the service\n",
    "```bash\n",
    "kubectl apply -f gateway-service.yaml\n",
    "kubectl port-forward service/gateway 8080:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Deploying to EKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a EKS cluster on AWS\n",
    "\n",
    "- Installing eksctl\n",
    "```bash\n",
    "eksctl create cluster -f ./kube-config/eks-config.yaml\n",
    "aws ecr create-repository --repository-name mlzoomcamp-images\n",
    "```\n",
    "\n",
    "- Publishing the image to ECR\n",
    "- Configuring kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-Serving is a system for deploying TensorFlow models\n",
    "- When using TF-Serving, we need a component for pre-processing\n",
    "- Kubernetes is a container orchestration platform\n",
    "- To deploy something on Kubernetes, we need to specify a deployment and a service\n",
    "- You can use Docker compose and Kind for local experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10 Explore more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other local Kuberneteses: minikube, k3d, k3s, microk8s, EKS Anywhere\n",
    "- [Rancher desktop](https://rancherdesktop.io/)\n",
    "- Docker desktop\n",
    "- [Lens](https://k8slens.dev/)\n",
    "- Many cloud providers have Kubernetes: GCP, Azure, Digital Ocean and others. Look for \"Managed Kubernetes\" in your favorite search engine\n",
    "- Deploy the model from previous modules and from your project with Kubernetes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FashionClassification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
