{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Kubernetes and TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We`ll deploy the clothes classification model we trained previously using Kubernetes and TensorFlow Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What we'll cover this week\n",
    "- Two-tier architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kubernetes overview](./images/kubernetes_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The saved_model format\n",
    "- Running TF-Serving locally with Docker\n",
    "- Invoking the model from Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 10:10:28.603762: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-11 10:10:28.603794: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-12-11 10:10:28.603800: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-12-11 10:10:28.604075: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-11 10:10:28.604464: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = keras.models.load_model(\"./models/xception_v4_large_08_0.894.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/clothing-model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/clothing-model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, \"./models/clothing-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m\u001b[39m\u001b[38;5;6m\u001b[1m clothing-model\u001b[0m\n",
      "\u001b[38;5;245m├── \u001b[39m\u001b[38;5;6m\u001b[1m assets\u001b[0m\n",
      "\u001b[38;5;245m├── \u001b[39m\u001b[38;5;6m\u001b[1m variables\u001b[0m\n",
      "\u001b[38;5;245m│   ├── \u001b[39m variables.data-00000-of-00001\n",
      "\u001b[38;5;245m│   └── \u001b[39m variables.index\n",
      "\u001b[38;5;245m├── \u001b[39m fingerprint.pb\n",
      "\u001b[38;5;245m└── \u001b[39m saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "!lsd --tree models/clothing-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_8'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 299, 299, 3)\n",
      "        name: serving_default_input_8:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_7'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "The MetaGraph with tag set ['serve'] contains the following ops: {'StringJoin', 'AssignVariableOp', 'AddV2', 'Mean', 'VarHandleOp', 'DisableCopyOnRead', 'Placeholder', 'RestoreV2', 'ShardedFilename', 'StatefulPartitionedCall', 'Const', 'SaveV2', 'MatMul', 'DepthwiseConv2dNative', 'Select', 'Conv2D', 'Relu', 'MaxPool', 'MergeV2Checkpoints', 'ReadVariableOp', 'FusedBatchNormV3', 'Pack', 'NoOp', 'StaticRegexFullMatch', 'Identity', 'BiasAdd'}\n",
      "2024-12-11 11:52:30.122000: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-11 11:52:30.122025: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-12-11 11:52:30.122033: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-12-11 11:52:30.122306: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-11 11:52:30.122376: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_8: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_8')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_8: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_8')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_8: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_8')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_8: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_8')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_8: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_8')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir clothing-model --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the second signature defined in the model `signature_def['serving_default']`, which is the one that takes an image as input and returns the class probabilities. see [Model description](./model-description.txt)\n",
    "\n",
    "Now we have to inject the parameters into a docker container\n",
    "\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "    --platform=linux/amd64 \\\n",
    "    -p 8500:8500 \\\n",
    "    -v \"$(pwd)/models/clothing-model:/models/clothing-model/1\" \\\n",
    "    -e MODEL_NAME=\"clothing-model\" \\\n",
    "    tensorflow/serving:2.7.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[!BUG]\n",
    ">\n",
    ">```plaintext\n",
    "> /usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if found this [hint](https://github.com/tensorflow/serving/issues/1816#issuecomment-2445056791)\n",
    "\n",
    "Docker release 4.35.0 (172550) for Mac introduces Docker VMM Beta, a replacement for the Apple Virtualisation Framework using Rosetta. Good news is that I can run the native TF Serving image now on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ! HELP: Does not run under Apple Silicon -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-image-helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost:8500'\n",
    "\n",
    "channel = grpc.insecure_channel(host)\n",
    "\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_image_helper import create_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = create_preprocessor('xception', target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://bit.ly/mlbookcamp-pants'\n",
    "X = preprocessor.from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_protobuf(data):\n",
    "    return tf.make_tensor_proto(data, shape=data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_request = predict_pb2.PredictRequest()\n",
    "\n",
    "pb_request.model_spec.name = 'clothing-model'\n",
    "pb_request.model_spec.signature_name = 'serving_default'\n",
    "\n",
    "pb_request.inputs[\"input_8\"].CopyFrom(np_to_protobuf(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_response = stub.Predict(pb_request, timeout=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pb_response.outputs['dense_7'].float_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -1.8798644542694092,\n",
       " 'hat': -4.756311416625977,\n",
       " 'longsleeve': -2.35953426361084,\n",
       " 'outwear': -1.0892632007598877,\n",
       " 'pants': 9.903782844543457,\n",
       " 'shirt': -2.8261797428131104,\n",
       " 'shoes': -3.6483113765716553,\n",
       " 'shorts': 3.2411556243896484,\n",
       " 'skirt': -2.6120963096618652,\n",
       " 't-shirt': -4.852035999298096}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(classes, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Creating a pre-processing service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting the notebook to a Python script\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to script 10_kubernetes.ipynb\n",
    "```\n",
    "=> saved as [gateway.py](./gateway.py)\n",
    "\n",
    "\n",
    "- Wrappping the script into a Flask app\n",
    "- Testing the service with [`test.py`](./test.py)\n",
    "\n",
    "```bash\n",
    "python test.py\n",
    "```\n",
    "\n",
    "- Putting everything into Pipenv\n",
    "\n",
    "```bash\n",
    "pip install pipenv\n",
    "pipenv install grpcio==1.42.0 flask gunicorn keras-image-helper\n",
    "pipenv install tensorflow-protobuf==2.7.0\n",
    "pipenv shell\n",
    "python gateway.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did not work out of the box. See [new env with py3.8](./environment_py38.yml). Still not locking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not drag 1,2GB TF into docker, we only need `TensorFlow Protobuf`, new file `proto.py`\n",
    "- [https://github.com/alexeygrigorev/tensorflow-protobuf](https://github.com/alexeygrigorev/tensorflow-protobuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Running everything locally with Docker-compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preparing tensorflow-serving image\n",
    "```bash\n",
    "docker build -t zoomcamp-10-model:xception-v4-001 -f image-model.dockerfile .\n",
    "```\n",
    "\n",
    "- on macOS, with Apple Silicon (arm64) we need to build the image for the correct platform\n",
    "\n",
    "```bash\n",
    "docker build --platform=linux/amd64 -t zoomcamp-10-model:xception-v4-001 -f image-model.dockerfile .\n",
    "```\n",
    "\n",
    "\n",
    "- Running tensorflow-serving image\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "    -p 8500:8500 \\\n",
    "    zoomcamp-10-model:xception-v4-001\n",
    "```\n",
    "\n",
    "- again on macOS, with Apple Silicon (arm64) we need to run the image for the correct platform\n",
    "\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "    --platform=linux/amd64 \\\n",
    "    -p 8500:8500 \\\n",
    "    zoomcamp-10-model:xception-v4-001\n",
    "```\n",
    "\n",
    "\n",
    "- Running the service, switch __main__ code in `gateway.py`\n",
    "```bash\n",
    "pipenv run python gateway.py\n",
    "```\n",
    "\n",
    "- Building the gateway image\n",
    "```bash\n",
    "docker build -t zoomcamp-10-gateway:001 -f image-gateway.dockerfile .\n",
    "```\n",
    "\n",
    "- Running the gateway\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "    -p 9696:9696 \\\n",
    "    zoomcamp-10-gateway:001\n",
    "```\n",
    "\n",
    "communication between the two containers is done via docker compose -> network bridge\n",
    "\n",
    "- Installing docker-compose on macOS with Homebrew, for other OS see [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)\n",
    "\n",
    "<img src=\"./images/docker_compose.png\" alt=\"docker-compose\" style=\"width:600px;height:auto;\">\n",
    "\n",
    "```bash\n",
    "brew install docker-compose\n",
    "```\n",
    "\n",
    "- Create `/bin`-folder in home directory\n",
    "- move into folder, download docker-compose\n",
    "- make it executable\n",
    "```bash\n",
    "mkdir ~/bin\n",
    "cd ~/bin\n",
    "curl -L <LINK> -o docker-compose\n",
    "chmod +x docker-compose\n",
    "```\n",
    "\n",
    "- Add to PATH\n",
    "```bash\n",
    "echo 'export PATH=\"${HOME}/bin:${PATH}\"' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "- Create `docker-compose.yml`\n",
    "```bash\n",
    "docker build -t zoomcamp-10-gateway:002 -f image-gateway.dockerfile .\n",
    "```\n",
    "\n",
    "- Running the service\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "- Testing the service\n",
    "```bash\n",
    "python test.py\n",
    "```\n",
    "\n",
    "- Ctrl+C to stop the service\n",
    "- detaching the service\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "- Stopping the service\n",
    "```bash\n",
    "docker-compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Introduction to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The anatomy of a Kubernetes cluster\n",
    "\n",
    "<img src=\"./images/intro_kubernetes.png\" alt=\"intro kubernetes\" style=\"width:600px;height:auto;\">\n",
    "\n",
    "<img src=\"./images/glossar.png\" alt=\"glossar\" style=\"width:600px;height:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Deploying a simple service to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create simple ping application in Flask\n",
    "```bash\n",
    "pipenv install flask gunicorn\n",
    "```\n",
    "\n",
    "✘ Locking Failed! Failed to lock Pipfile.lock!\n",
    "on macOS\n",
    "\n",
    "- Installing kubectl - installed with docker/docker-desktop\n",
    "\n",
    "- Setting up a local Kubernetes cluster with Kind\n",
    "```bash\n",
    "brew install kind\n",
    "kind create cluster\n",
    "kubectl cluster-info --context kind-kind\n",
    "kubectl get service\n",
    "kubectl get pods\n",
    "kubectl get deployments\n",
    "```\n",
    "\n",
    "- Creating a deployment\n",
    "    - [deployment.yaml](./deployment.yaml)\n",
    "```bash\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl get deployment\n",
    "kubectl get pod\n",
    "kubectl describe pod <pod-name>\n",
    "kind load docker-image ping:v001\n",
    "kubectl get pod\n",
    "kubectl port-forward <pod-name> 9696:9696\n",
    "curl localhost:9696/ping\n",
    "```\n",
    "\n",
    "- Creating a service\n",
    "  - [service.yaml](./service.yaml)\n",
    "```bash\n",
    "kubectl apply -f service.yaml\n",
    "kubectl get service\n",
    "kubectl get svc\n",
    "```\n",
    "\n",
    "- change service type to `LoadBalancer` in `service.yaml`\n",
    "```bash\n",
    "kubectl apply -f service.yaml\n",
    "kubectl get service\n",
    "kubectl port-forward service/ping 8080:80\n",
    "curl localhost:8080/ping\n",
    "```\n",
    "\n",
    "<img src=\"./images/port_forwarding.png\" alt=\"port forwarding\" style=\"width:600px;height:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Deploying TensorFlow models to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deploying the TF-Serving model\n",
    "\n",
    "```bash\n",
    "kind load docker-image zoomcamp-10-model:xception-v4-001\n",
    "cd kube-config\n",
    "kubectl apply -f model-deployment.yaml\n",
    "kubectl get pod\n",
    "kubectl port-forward <pod-name> 8500:8500\n",
    "kubectl apply -f model-service.yaml\n",
    "kubectl get service\n",
    "kubectl port-forward service/tf-serving-clothing-model 8500:8500\n",
    "```\n",
    "\n",
    "- Deploying the Gateway\n",
    "\n",
    "```bash\n",
    "kind load docker-image zoomcamp-10-gateway:002\n",
    "kubectl apply -f gateway-deployment.yaml\n",
    "```\n",
    "\n",
    "- Testing the service\n",
    "```bash\n",
    "kubectl get pod\n",
    "kubectl exec -it <pod-name> -- /bash\n",
    "```\n",
    "\n",
    "- on pod\n",
    "```bash\n",
    "curl localhost:9696/ping\n",
    "```\n",
    "\n",
    "- Exposing the service\n",
    "```bash\n",
    "kubectl apply -f gateway-service.yaml\n",
    "kubectl port-forward service/gateway 8080:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Deploying to EKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a EKS cluster on AWS\n",
    "\n",
    "- Installing eksctl\n",
    "```bash\n",
    "eksctl create cluster -f ./kube-config/eks-config.yaml\n",
    "aws ecr create-repository --repository-name mlzoomcamp-images\n",
    "```\n",
    "\n",
    "- Publishing the image to ECR\n",
    "- Configuring kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-Serving is a system for deploying TensorFlow models\n",
    "- When using TF-Serving, we need a component for pre-processing\n",
    "- Kubernetes is a container orchestration platform\n",
    "- To deploy something on Kubernetes, we need to specify a deployment and a service\n",
    "- You can use Docker compose and Kind for local experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10 Explore more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other local Kuberneteses: minikube, k3d, k3s, microk8s, EKS Anywhere\n",
    "- [Rancher desktop](https://rancherdesktop.io/)\n",
    "- Docker desktop\n",
    "- [Lens](https://k8slens.dev/)\n",
    "- Many cloud providers have Kubernetes: GCP, Azure, Digital Ocean and others. Look for \"Managed Kubernetes\" in your favorite search engine\n",
    "- Deploy the model from previous modules and from your project with Kubernetes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FashionClassification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
