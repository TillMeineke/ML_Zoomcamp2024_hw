{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras                     2.14.0             pyhd8ed1ab_0    conda-forge\n",
      "keras-image-helper        0.0.1                    pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "!conda list | grep \"keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras                   2.14.0\n",
      "keras-image-helper      0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep \"keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version of TF should be above 2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.10\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# !wget https://www.github.com/alexeygrigorev/mlbookcamp-code/releases/download/chapter-7-model/xception_v4_large_08_0.894.h5 -O clothing-model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-23 02:05:07--  http://bit.ly/mlbookcamp-pants\n",
      "Auflösen des Hostnamens bit.ly (bit.ly)… 67.199.248.10, 67.199.248.11\n",
      "Verbindungsaufbau zu bit.ly (bit.ly)|67.199.248.10|:80 … verbunden.\n",
      "HTTP-Anforderung gesendet, auf Antwort wird gewartet … 301 Moved Permanently\n",
      "Platz: https://raw.githubusercontent.com/alexeygrigorev/clothing-dataset-small/master/test/pants/4aabd82c-82e1-4181-a84d-d0c6e550d26d.jpg [folgend]\n",
      "--2024-09-23 02:05:08--  https://raw.githubusercontent.com/alexeygrigorev/clothing-dataset-small/master/test/pants/4aabd82c-82e1-4181-a84d-d0c6e550d26d.jpg\n",
      "Auflösen des Hostnamens raw.githubusercontent.com (raw.githubusercontent.com)… 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Verbindungsaufbau zu raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443 … verbunden.\n",
      "HTTP-Anforderung gesendet, auf Antwort wird gewartet … 200 OK\n",
      "Länge: 23048 (23K) [image/jpeg]\n",
      "Wird in »./images/pants.jpg« gespeichert.\n",
      "\n",
      "./images/pants.jpg  100%[===================>]  22,51K  --.-KB/s    in 0,002s  \n",
      "\n",
      "2024-09-23 02:05:08 (12,1 MB/s) - »./images/pants.jpg« gespeichert [23048/23048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load sample image\n",
    "# !wget http://bit.ly/mlbookcamp-pants -O ./images/pants.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.applications.xception import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 07:46:03.077403: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-12-04 07:46:03.077477: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-12-04 07:46:03.077490: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-12-04 07:46:03.077547: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-04 07:46:03.077568: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('./models/xception_v4_08_0.868.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('./images/pants.jpg', target_size=(299, 299))\n",
    "\n",
    "x = np.array(img)\n",
    "X = np.array([x])\n",
    "\n",
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 07:46:11.060567: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.1288574 , -4.6540356 , -1.1132694 , -0.20497966,  9.407786  ,\n",
       "        -0.106648  , -5.7733626 ,  2.9702551 , -1.4123498 , -4.4523616 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -2.1288574,\n",
       " 'hat': -4.6540356,\n",
       " 'longsleeve': -1.1132694,\n",
       " 'outwear': -0.20497966,\n",
       " 'pants': 9.407786,\n",
       " 'shirt': -0.106648,\n",
       " 'shoes': -5.7733626,\n",
       " 'shorts': 2.9702551,\n",
       " 'skirt': -1.4123498,\n",
       " 't-shirt': -4.4523616}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(classes, preds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing new so far, just testing if it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Keras to TF-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/r8/zdlnr35s6qz6zx67nmc9bdnm0000gn/T/tmpgx4moqy0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/r8/zdlnr35s6qz6zx67nmc9bdnm0000gn/T/tmpgx4moqy0/assets\n",
      "2024-12-04 07:46:36.854461: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-12-04 07:46:36.854649: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-12-04 07:46:36.855211: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/r8/zdlnr35s6qz6zx67nmc9bdnm0000gn/T/tmpgx4moqy0\n",
      "2024-12-04 07:46:36.867718: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-12-04 07:46:36.867736: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/r8/zdlnr35s6qz6zx67nmc9bdnm0000gn/T/tmpgx4moqy0\n",
      "2024-12-04 07:46:36.892406: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "2024-12-04 07:46:36.904190: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-12-04 07:46:37.280860: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/r8/zdlnr35s6qz6zx67nmc9bdnm0000gn/T/tmpgx4moqy0\n",
      "2024-12-04 07:46:37.387589: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 532378 microseconds.\n",
      "2024-12-04 07:46:37.516737: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('./models/clothing_model.tflite', 'wb') as f_out:\n",
    "    f_out.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.lite as tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allocate the memory for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initialized TensorFlow Lite runtime.\n",
      "INFO: Applying 1 TensorFlow Lite delegate(s) lazily.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "VERBOSE: Replacing 104 out of 104 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 1 partitions for the whole graph.\n",
      "INFO: Successfully applied the default TensorFlow Lite delegate indexed at 0.\n",
      " *NOTE*: because a delegate has been applied, the precision of computations should be unchanged, but the exact output tensor values may have changed. If such output values are checked in your code, like in your tests etc., please consider increasing error tolerance for the check.\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and get indices of input and output tensors\n",
    "interpreter = tflite.Interpreter(\"./models/clothing_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "invoke the converter and fetch the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencing\n",
    "interpreter.set_tensor(input_index, X)\n",
    "interpreter.invoke()\n",
    "\n",
    "preds = interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -2.2917588,\n",
       " 'hat': -2.2173786,\n",
       " 'longsleeve': -1.9650993,\n",
       " 'outwear': -1.4446676,\n",
       " 'pants': 7.1728725,\n",
       " 'shirt': -0.056686,\n",
       " 'shoes': -4.422337,\n",
       " 'shorts': 2.8628511,\n",
       " 'skirt': -2.050411,\n",
       " 't-shirt': -3.4516563}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post-processing: get the class name of the highest probability\n",
    "\n",
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "dict(zip(classes, preds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we used TensorFlow Lite to make the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing TF dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Image.open('./images/pants.jpg') as img:\n",
    "    img = img.resize((299, 299), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    x /= 127.5\n",
    "    x -= 1.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(img, dtype=\"float32\")\n",
    "X = np.array([x])\n",
    "\n",
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencing\n",
    "interpreter.set_tensor(input_index, X)\n",
    "interpreter.invoke()\n",
    "preds = interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -2.2917588,\n",
       " 'hat': -2.2173786,\n",
       " 'longsleeve': -1.9650993,\n",
       " 'outwear': -1.4446676,\n",
       " 'pants': 7.1728725,\n",
       " 'shirt': -0.056686,\n",
       " 'shoes': -4.422337,\n",
       " 'shorts': 2.8628511,\n",
       " 'skirt': -2.050411,\n",
       " 't-shirt': -3.4516563}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post-processing: get the class name of the highest probability\n",
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "dict(zip(classes, preds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpler way of doing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- go to Alexey's github profile\n",
    "- open repositories\n",
    "- look for [\"keras-image-helper\"](https://github.com/alexeygrigorev/keras-image-helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-image-helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://google-coral.github.io/py-repo/\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tflite_runtime\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not use the TF Lite module from TensorFlow, we install the tflite_runtime package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list | grep \"tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import tensorflow.lite as tflite\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtflite_runtime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtflite\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_image_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_preprocessor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'"
     ]
    }
   ],
   "source": [
    "# import tensorflow.lite as tflite\n",
    "import tflite_runtime.interpreter as tflite\n",
    "from keras_image_helper import create_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and get indices of input and output tensors\n",
    "interpreter = tflite.Interpreter(\"./models/clothing_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = create_preprocessor('xception', target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://bit.ly/mlbookcamp-pants'\n",
    "X = preprocessor.from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_index, X)\n",
    "interpreter.invoke()\n",
    "preds = interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing: get the class name of the highest probability\n",
    "classes = [\n",
    "    'dress',\n",
    "    'hat',\n",
    "    'longsleeve',\n",
    "    'outwear',\n",
    "    'pants',\n",
    "    'shirt',\n",
    "    'shoes',\n",
    "    'shorts',\n",
    "    'skirt',\n",
    "    't-shirt'\n",
    "]\n",
    "\n",
    "dict(zip(classes, preds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert this notebook to a Python script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FashionClassification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
